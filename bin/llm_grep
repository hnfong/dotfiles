#!/usr/bin/env python3

"""
This program constructs a prompt based on a query and input lines, then uses an
LLM (via `ask.py`) to process the prompt, for filtering or searching content in
the input lines.

Usage:

    llm_grep [-m model] QUERY [FILE]

"""


import sys
import subprocess
import getopt
import tempfile

DEFAULT_MODEL_NAME = "gemma-3-12b" # For ask.py

TEMPLATE = """
# Task description
I want you to perform a semantic "grep" given the input and a query/description.

The input is a text file (or a segment of it) with line numbers (the line
numbers may not start with 1 if it is a segment). You are required to give an
appropriate output given the description below.

For every line in the input, output the original line (together with line
number) if and only if the line has something that matches the given query
(below), otherwise ignore the line. Only trigger when the line obviously
matches the query, i.e. not tangentially related. Consider each line
separately, do not take into account surrounding lines.

# Query description

User's query: " ${QUERY} "

# START OF INPUT

${LINES}

# END OF INPUT

For every line in the input, output the original line (together with line
number) if and only if the line has something that matches the given query
(below), otherwise ignore the line. Only trigger when the line obviously
matches the query, i.e. not tangentially related. Consider each line
separately, do not take into account surrounding lines.

** Reminder, this is what we want: `${QUERY}` **

If no lines match just give empty response. Don't explain. Just output as if
you're a command line program called "semantic_grep".

Important: after each line of output, also emit a line of "---" to separate it
from your next output.
"""

def usage():
    sys.stderr.write(__doc__.strip())
    sys.stderr.write("\n")

def iterator_n_lines(f, n):
    ret = []
    for _ in range(n):
        line = f.readline()
        if len(line):
            ret.append(line)
        else:
            break
    return ret

def create_prompt(query, inputs, starting_line = 1):
    prompt = TEMPLATE.replace("${QUERY}", query)

    lines_buf = []
    for line_num, line in enumerate(inputs):
        line_num += starting_line
        lines_buf.append(f"{line_num}:{line.rstrip()}")

    return prompt.replace("${LINES}", "\n".join(lines_buf))

def main():
    try:
        opt_list, args = getopt.getopt(sys.argv[1:], "m:h", [])
    except getopt.GetoptError as err:
        usage()
        sys.exit(1)

    if len(args) < 1:
        usage()
        sys.exit(1)

    opts = dict(opt_list)

    model = opts.get("-m", DEFAULT_MODEL_NAME)
    query = args[0]

    if len(args) > 1:
        input_file = open(args[1], "rt")
    else:
        input_file = sys.stdin

    chunk_size = 100
    starting_line = 1 - chunk_size
    while (chunk := iterator_n_lines(input_file, chunk_size)):
        starting_line += chunk_size
        prompt = create_prompt(query, chunk, starting_line=starting_line)

        with tempfile.NamedTemporaryFile(mode='w') as temp_file:
            temp_filename = temp_file.name

            temp_file.write(prompt)
            temp_file.flush()

            # Call the LLM using ask.py
            cmd = ["ask.py", "-m", model, "-c", "9999", "-d", "4096", "-f", temp_filename]  # 262144 is for Qwen3-30B-A3B-Instruct
            # print(prompt)
            print(cmd)
            subprocess.run(cmd, check=True)

if __name__ == "__main__":
    main()
